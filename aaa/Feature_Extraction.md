  前略：[Paddle 入门](https://saaavsaaa.github.io/aaa/Paddle_Begin.html) 中用了线性回归的模型，模型本身十分简单，那么用于训练的输入数据就十分重要，而手头上有的数据也是各种各样，对数据的特征提取对于此类模型十分重要，我感觉也是真正开始的第一步(似乎应该先做一下探索性数据分析，特别是对业务不熟悉的情况下，不过回头再说)，虽然我开始都直接用的占比。     
  对不同类型数据的提取：     
    数值类型：     
      截断:降低精度；     
      二值化:由于计数累加可能极快。处理计数特征时首先要考试是保留原始计数、转为二值变量来标识是否存在或者进行分桶。     
      分桶:当数值跨越不同的数量级。有些模型对比较大的数值敏感，这种情况下常见的方法就是分桶，可以固定宽度均匀分桶，可以根据10的幂次，也可以基于数据分布或使用模型例如聚类的方式将特征分为多个类别。由于计算是对桶进行的，系数是根据桶运算出的，特征的表现形式，分桶并不会影响正确性。     
      缩放:将数据按比例缩放，使之落入一个小的特定区间。
           标准化缩放(z-score):一个分数与平均数的差再除以标准差，公式为z=(x-μ)/σ。其中x为某一具体分数， μ为平均数，σ为标准差。其中μ为总体平均值，x-μ为离均差，σ表示总体标准偏差，Z值的量代表着原始分数和母体平均值之间的距离，是以标准差为单位计算的 —— 一个给定分数距离平均数多少个标准差。当原始分数低于平均值时，z为负，高于平局值为正。一个数列的各z分数的平方和等于该数列数据的个数，并且z分数的标准差和方差都为1.平均数为0。z分数是一种可以看出某分数在分布中相对位置的方法。将成正态分布的数据中的原始分数转换为z分数，就可以通过查阅z分数在正态曲线下面积的情况来得知平均数与z分数之间的面积，进而得知原始分数在数据集合中的百分等级。适用于目标变量为输入特征的光滑函数的模型,线性回归、逻辑回归等。归一化方式要求原始数据的分布可以近似为高斯分布，在不涉及距离度量、协方差计算、数据不符合正太分布时效果不佳。     
           最大最小值归一化:将原始数据线性化的方法转换到[0，1]的范围，公式X<sub>normalize</sub>=(X-X<sub>min</sub>) / (X<sub>max</sub>-X)。缺点是抗干扰能力弱，受离群值影响比较大，会有缺失数据，有新的输入值时可能会受到输入值影响。最大绝对值归一化，使用绝对值最大的数除。还有基于范数的归一化，如L0范数是指向量中非0的元素的个数；L1 范数(向量中各个元素绝对值之和，L1范数是L0范数的最优凸近似。任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。W的L1范数是绝对值，|w|在w=0处是不可微。L0也可以实现稀疏，但是实际中会使用L1取代L0。在支持向量机（support vector machine）学习过程中，实际是一种对于成本函数(cost function)求解最优的过程，因此，L1范数正则化通过向成本函数中添加L1范数，使得学习得到的结果满足稀疏化(sparsity)，从而方便人们提取特征);L2范数是指向量中各元素的平方和然后开方。L1会趋向于产生少量的特征，其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。越小的参数说明模型越简单，越简单的模型则越不容易过拟合。

  前略：[Paddle 入门](https://saaavsaaa.github.io/aaa/Paddle_Begin.html) 中用了线性回归的模型，模型本身十分简单，那么用于训练的输入数据就十分重要，而手头上有的数据也是各种各样，对数据的特征提取对于此类模型十分重要，我感觉也是真正开始的第一步(似乎应该先做一下探索性数据分析，特别是对业务不熟悉的情况下，不过回头再说)，虽然我开始都直接用的占比。以下内容来源于《美团机器学习实践》，百度百科，以及各种没记下来来源处的阅读笔记。     

  对不同类型数据的提取，具体采用哪种方式依赖于业务、数据和所选取的模型，首先要理解**数据**和**业务逻辑以**及**模型特点**才能更好的进行特征工程：     
  
#  (还需要介绍特征矩阵)

### 数值特征：   

#### 截断:降低精度；   

##### 二值化:
由于计数累加可能极快。处理计数特征时首先要考试是保留原始计数、转为二值变量来标识是否存在或者进行分桶。     
##### 分桶:
当数值跨越不同的数量级。有些模型对比较大的数值敏感，这种情况下常见的方法就是分桶，可以固定宽度均匀分桶，可以根据10的幂次，也可以基于数据分布或使用模型例如聚类的方式将特征分为多个类别。由于计算是对桶进行的，系数是根据桶运算出的，特征的表现形式，分桶并不会影响正确性。     
##### 缩放:
将数据按比例缩放，使之落入一个小的特定区间。

  标准化缩放(z-score):一个分数与平均数的差再除以标准差，公式为z=(x-μ)/σ。其中x为某一具体分数， μ为平均数，σ为标准差。其中μ为总体平均值，x-μ为离均差，σ表示总体标准偏差，Z值的量代表着原始分数和母体平均值之间的距离，是以标准差为单位计算的 —— 一个给定分数距离平均数多少个标准差。当原始分数低于平均值时，z为负，高于平局值为正。一个数列的各z分数的平方和等于该数列数据的个数，并且z分数的标准差和方差都为1.平均数为0。z分数是一种可以看出某分数在分布中相对位置的方法。将成正态分布的数据中的原始分数转换为z分数，就可以通过查阅z分数在正态曲线下面积的情况来得知平均数与z分数之间的面积，进而得知原始分数在数据集合中的百分等级。适用于目标变量为输入特征的光滑函数的模型,线性回归、逻辑回归等。归一化方式要求原始数据的分布可以近似为高斯分布，在不涉及距离度量、协方差计算、数据不符合正太分布时效果不佳。    
  最大最小值归一化:将原始数据线性化的方法转换到[0，1]的范围，公式X<sub>normalize</sub>=(X-X<sub>min</sub>) / (X<sub>max</sub>-X)。缺点是抗干扰能力弱，受离群值影响比较大，会有缺失数据，有新的输入值时可能会受到输入值影响。最大绝对值归一化，使用绝对值最大的数除。还有移动小数点的小数定标方法也是根据最大值绝对值，最大值绝对值多少位就移动多少位。     
  还有基于范数的归一化，如L0范数是指向量中非0的元素的个数；L1 范数(向量中各个元素绝对值之和，L1范数是L0范数的最优凸近似。任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。W的L1范数是绝对值，|w|在w=0处是不可微。L0也可以实现稀疏，但是实际中会使用L1取代L0。在支持向量机（support vector machine）学习过程中，实际是一种对于成本函数(cost function)求解最优的过程，因此，L1范数正则化通过向成本函数中添加L1范数，使得学习得到的结果满足稀疏化(sparsity)，从而方便人们提取特征);L2范数是指向量中各元素的平方和然后开方。L1会趋向于产生少量的特征，其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。越小的参数说明模型越简单，越简单的模型则越不容易过拟合(在对模型进行训练时，有可能遇到训练数据不够，即训练数据无法对整个数据的分布进行估计的时候，或者在对模型进行过度训练（overtraining）时，常常会导致模型的过拟合（overfitting），即模型复杂度比实际数据复杂度还要高)。     
  平方根缩放或对数缩放:对数缩放对于处理长尾分布(齐普夫定律，不太理解回头实际做做看)且取值为正数的数值变量非常有效，它将大端长尾压缩为短尾，并将小端进行延伸，平方根或者对数变换是幂变换的特例，在统计学中都称为方差稳定的变换，其中Box-Cox变换是简单的幂变换，Box-Cox转换仅对取值为正数的数值变量起作用。Box-Cox的一般形式是个分段函数，当λ=0时，y(λ)=lny，当λ≠0时y(λ)=(y<sup>λ</sup>-1)/λ，y(λ)为经Box-Cox变换后得到的新变量，y为原始连续因变量，λ为变换参数。以上变换要求原始变量y取值为正，若取值为负时，可先对所有原始数据同加一个常数a使其(y+a)为正值，然后再进行以上的变换。对不同的λ所作的变换不同。在λ=0时该变换为对数变换,通常用于呈正偏分布的数据，其中有些值非常大，如果这些大值位于研究区域中，对数变换有助于使方差更加恒定和归一化数据;λ=-1时为倒数变换;而在λ=0.5时为平方根变换。Box-Cox通常会使数据呈正态分布。Box-Cox变换中参数λ的估计有两种方法：(1)最大似然估计；(2)Bayes方法。通过求解λ值，就可以确定具体采用哪种变换形式。(这一段参考的比较杂，没有实际应用过，没什么感觉)    
  对于有异常点的数据，还可以使用中位数而不是均值，基于分位数而不基于方差(分位数亦称分位点，是指将一个随机变量的概率分布范围分为几个等份的数值点，常用的有中位数即二分位数、四分位数、百分位数等)。
##### 缺失值处理:
实际中经常会遇到缺失值的情况，很多模型无法处理特征缺失的情况，缺失特征的处理方式会影响模型效果。处理方法主要有两种：一是补一个值，比如均值，对于包含异常值的变量补一个中位数效果会更好，还可以用模型预测缺失值；另外一种是将缺失值当成一种特征信息编码让模型学习。也有模型是可以处理缺失特征的，如XGBoost。 
##### 特征交叉:
可以表示特征之间的相互作用，如对两个数值变量进行加减乘除等运算，可以根据统计校验或者特征对模型的重要性选择游泳的交叉组合。好处是可以在线性模型中引入非线性性质，提升模型的表达能力。如FM和FFM模型可以自动进行特征交叉组合。模型的表达能力用来衡量参数化模型可以拟合的函数的复杂程度，对什么样的参数有什么程度的区别。
##### 非线性编码:
线性模型很难学习到数据中的非线性效果，除了特征交叉外还可以通过非线性编码来提升线性模型的效果。如多项式核(K(x,xi)=(x▪xi+1)^d, d=1,2,...,N)、高斯核(k(||x-xc||)=exp{- ||x-xc||^2/(2 * σ^2) } 其中xc为核函数中心,σ为函数的宽度参数)等，合适的核函数不容易确定(核函数利用维度变化求解)。另外一种方法是将随机深林模型的叶节点进行编码喂给线性模型。还有基因算法(参见:[genetic](https://github.com/saaavsaaa/collection/tree/master/Algorithm))、局部线性嵌入(LLE:Locally linear embedding,在数据降维后仍然保留原始高维数据的拓扑结构，即数据点的局部邻接关系。首先找到每个数据点的k个最近邻，然后将当前数据点用k个最近邻线性表出，保证降维后还能保持这个线性关系)、谱嵌入和t-SNE等。包括上面核函数等放这也不合适，已经占了很多篇幅了，回头分别整理。
##### 行统计量:
除了对值变量处理外，还可以直接统计行向量，如行向量的空置个数、0的个数、正数或负数的个数，以及均值、方差、最大最小值、偏度、峰度等。

### 类别特征：
可以由原始数据提取，也可通过数值特征离散化得到：     
#### 自然数编码
#### 独热编码
#### 分层编码
#### 散列编码
#### 计数编码
#### 计数排名编码
#### 目标编码
#### 类别特征之间的交叉组合
#### 类别特征和数值特征之间的交叉组合

### 时间特征：

### 空间特征：

### 文本特征：
文本特征往往产生特别稀疏的特征矩阵，可以从几个方面对文本特征进行预处理：将字符转为小写、分词、去除无用字符、提取词根、拼写纠错、词干提取、标点符号编码、文档特征、实体插入和提取、Word2Vec、文本相似性、去除停止词、去除稀疏词、TF-IDF、LDA、LSA等。     
#### 语料构建
#### 文本清洗

#### 分词
##### 词性标注
##### 词形还原和词干提取
##### 文本统计特征
##### N-Gram模型

#### Skip-Gram模型
##### 词集模型
##### TF-IDF

#### 余弦相似度
#### Jaccard相似度
#### Levenshtein(编辑距离)
#### 隐性语义分析
#### Word2Vec

[edit](https://github.com/saaavsaaa/saaavsaaa.github.io/edit/master/aaa/Feature_Extraction.md)

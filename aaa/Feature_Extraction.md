  前略：[Paddle 入门](https://saaavsaaa.github.io/aaa/Paddle_Begin.html) 中用了线性回归的模型，模型本身十分简单，那么用于训练的输入数据就十分重要，而手头上有的数据也是各种各样，对数据的特征提取对于此类模型十分重要，我感觉也是真正开始的第一步(似乎应该先做一下探索性数据分析，特别是对业务不熟悉的情况下，不过回头再说)，虽然我开始都直接用的占比。以下内容来源于《美团机器学习实践》，百度百科，以及各种没记下来来源处的阅读笔记。     

  对不同类型数据的提取，具体采用哪种方式依赖于业务、数据和所选取的模型，首先要理解**数据**和**业务逻辑以**及**模型特点**才能更好的进行特征工程：     
  
#  (还需要介绍特征矩阵)

### 数值特征：   

#### 截断:降低精度；   

##### 二值化:
由于计数累加可能极快。处理计数特征时首先要考试是保留原始计数、转为二值变量来标识是否存在或者进行分桶。     
##### 分桶:
当数值跨越不同的数量级。有些模型对比较大的数值敏感，这种情况下常见的方法就是分桶，可以固定宽度均匀分桶，可以根据10的幂次，也可以基于数据分布或使用模型例如聚类的方式将特征分为多个类别。由于计算是对桶进行的，系数是根据桶运算出的，特征的表现形式，分桶并不会影响正确性。     
##### 缩放:
将数据按比例缩放，使之落入一个小的特定区间。

  标准化缩放(z-score):一个分数与平均数的差再除以标准差，公式为z=(x-μ)/σ。其中x为某一具体分数， μ为平均数，σ为标准差。其中μ为总体平均值，x-μ为离均差，σ表示总体标准偏差，Z值的量代表着原始分数和母体平均值之间的距离，是以标准差为单位计算的 —— 一个给定分数距离平均数多少个标准差。当原始分数低于平均值时，z为负，高于平局值为正。一个数列的各z分数的平方和等于该数列数据的个数，并且z分数的标准差和方差都为1.平均数为0。z分数是一种可以看出某分数在分布中相对位置的方法。将成正态分布的数据中的原始分数转换为z分数，就可以通过查阅z分数在正态曲线下面积的情况来得知平均数与z分数之间的面积，进而得知原始分数在数据集合中的百分等级。适用于目标变量为输入特征的光滑函数的模型,线性回归、逻辑回归等。归一化方式要求原始数据的分布可以近似为高斯分布，在不涉及距离度量、协方差计算、数据不符合正太分布时效果不佳。    
  最大最小值归一化:将原始数据线性化的方法转换到[0，1]的范围，公式X<sub>normalize</sub>=(X-X<sub>min</sub>) / (X<sub>max</sub>-X)。缺点是抗干扰能力弱，受离群值影响比较大，会有缺失数据，有新的输入值时可能会受到输入值影响。最大绝对值归一化，使用绝对值最大的数除。还有移动小数点的小数定标方法也是根据最大值绝对值，最大值绝对值多少位就移动多少位。     
  还有基于范数的归一化，如L0范数是指向量中非0的元素的个数；L1 范数(向量中各个元素绝对值之和，L1范数是L0范数的最优凸近似。任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。W的L1范数是绝对值，|w|在w=0处是不可微。L0也可以实现稀疏，但是实际中会使用L1取代L0。在支持向量机（support vector machine）学习过程中，实际是一种对于成本函数(cost function)求解最优的过程，因此，L1范数正则化通过向成本函数中添加L1范数，使得学习得到的结果满足稀疏化(sparsity)，从而方便人们提取特征);L2范数是指向量中各元素的平方和然后开方。L1会趋向于产生少量的特征，其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。越小的参数说明模型越简单，越简单的模型则越不容易过拟合(在对模型进行训练时，有可能遇到训练数据不够，即训练数据无法对整个数据的分布进行估计的时候，或者在对模型进行过度训练（overtraining）时，常常会导致模型的过拟合（overfitting），即模型复杂度比实际数据复杂度还要高)。     
  平方根缩放或对数缩放:对数缩放对于处理长尾分布(齐普夫定律，不太理解回头实际做做看)且取值为正数的数值变量非常有效，它将大端长尾压缩为短尾，并将小端进行延伸，平方根或者对数变换是幂变换的特例，在统计学中都称为方差稳定的变换，其中Box-Cox变换是简单的幂变换，Box-Cox转换仅对取值为正数的数值变量起作用。Box-Cox的一般形式是个分段函数，当λ=0时，y(λ)=lny，当λ≠0时y(λ)=(y<sup>λ</sup>-1)/λ，y(λ)为经Box-Cox变换后得到的新变量，y为原始连续因变量，λ为变换参数。以上变换要求原始变量y取值为正，若取值为负时，可先对所有原始数据同加一个常数a使其(y+a)为正值，然后再进行以上的变换。对不同的λ所作的变换不同。在λ=0时该变换为对数变换,通常用于呈正偏分布的数据，其中有些值非常大，如果这些大值位于研究区域中，对数变换有助于使方差更加恒定和归一化数据;λ=-1时为倒数变换;而在λ=0.5时为平方根变换。Box-Cox通常会使数据呈正态分布。Box-Cox变换中参数λ的估计有两种方法：(1)最大似然估计；(2)Bayes方法。通过求解λ值，就可以确定具体采用哪种变换形式。(这一段参考的比较杂，没有实际应用过，没什么感觉)    
  对于有异常点的数据，还可以使用中位数而不是均值，基于分位数而不基于方差(分位数亦称分位点，是指将一个随机变量的概率分布范围分为几个等份的数值点，常用的有中位数即二分位数、四分位数、百分位数等)。
##### 缺失值处理:
实际中经常会遇到缺失值的情况，很多模型无法处理特征缺失的情况，缺失特征的处理方式会影响模型效果。处理方法主要有两种：一是补一个值，比如均值，对于包含异常值的变量补一个中位数效果会更好，还可以用模型预测缺失值；另外一种是将缺失值当成一种特征信息编码让模型学习。也有模型是可以处理缺失特征的，如XGBoost。 
##### 特征交叉:
可以表示特征之间的相互作用，如对两个数值变量进行加减乘除等运算，可以根据统计校验或者特征对模型的重要性选择游泳的交叉组合。好处是可以在线性模型中引入非线性性质，提升模型的表达能力。如FM和FFM模型可以自动进行特征交叉组合。模型的表达能力用来衡量参数化模型可以拟合的函数的复杂程度，对什么样的参数有什么程度的区别。
##### 非线性编码:
线性模型很难学习到数据中的非线性效果，除了特征交叉外还可以通过非线性编码来提升线性模型的效果。如多项式核(K(x,xi)=(x▪xi+1)^d, d=1,2,...,N)、高斯核(k(||x-xc||)=exp{- ||x-xc||^2/(2 * σ^2) } 其中xc为核函数中心,σ为函数的宽度参数)等，合适的核函数不容易确定(核函数利用维度变化求解)。另外一种方法是将随机深林模型的叶节点进行编码喂给线性模型。还有基因算法(参见:[genetic](https://github.com/saaavsaaa/collection/tree/master/Algorithm))、局部线性嵌入(LLE:Locally linear embedding,在数据降维后仍然保留原始高维数据的拓扑结构，即数据点的局部邻接关系。首先找到每个数据点的k个最近邻，然后将当前数据点用k个最近邻线性表出，保证降维后还能保持这个线性关系)、谱嵌入和t-SNE等。包括上面核函数等放这也不合适，已经占了很多篇幅了，回头分别整理。
##### 行统计量:
除了对值变量处理外，还可以直接统计行向量，如行向量的空置个数、0的个数、正数或负数的个数，以及均值、方差、最大最小值、偏度、峰度等。

### 类别特征：
可以由原始数据提取，也可通过数值特征离散化得到：     
#### 自然数编码
#### 独热编码
#### 分层编码
#### 散列编码
#### 计数编码
#### 计数排名编码
#### 目标编码
#### 类别特征之间的交叉组合
#### 类别特征和数值特征之间的交叉组合

### 时间特征：

### 空间特征：

### 文本特征：
文本特征往往产生特别稀疏的特征矩阵，可以从几个方面对文本特征进行预处理：将字符转为小写、分词、去除无用字符、提取词根、拼写纠错、词干提取、标点符号编码、文档特征、实体插入和提取、Word2Vec、文本相似性、去除停止词、去除稀疏词、TF-IDF、LDA、LSA等。     
#### 语料构建
#### 文本清洗

#### 分词
##### 词性标注
##### 词形还原和词干提取
##### 文本统计特征
##### N-Gram模型

#### Skip-Gram模型
##### 词集模型
##### TF-IDF

#### 余弦相似度
#### Jaccard相似度
#### Levenshtein(编辑距离)
#### 隐性语义分析
#### Word2Vec

类别特征一般首先要转换成数值类型才能使用，最简单的编码方式是一个类别给一个编号，对类别编号进行洗牌，训练多个模型进行融合可以一步提升模型效果。

通常直接用自然数编码效果不好，尤其是线性模型。这是因为对于自然数编码，数值大小没有物理含义，直接给线性模型没有意义。通常是对类别特征进行独热编码，这样每个特征取值可以对应所有特征，得到稀疏的特征矩阵。有多少个状态就有多少比特，而且只有一个比特为1，其他全为0的一种码制，与位图十分相似

对于邮编或身份证号登可以取不同位数进行分层，然后按层次进行自然数编码

对于有些取值特别多的类别特征使用独热得到的矩阵会过于稀疏，可以在独热编码前先进行散列编码，可以避免特征矩阵过于习俗。实际使用中可重复多次选取不同的散列函数，利用融合方式提升模型效果。散列方法可能会导致特征取值冲突，这种冲突会削弱模型的效果。自然数编码和分层编码可以看做是散列的特例。

计数编码是讲类别特征用其对应的计数来代替，这种方法对异常值比较敏感，特征取值也可能冲突

利用计数的排名进行编码，比计数编码更健壮，也不会冲突

基于目标变量对类别特征进行编码，对于基数（类别变量所有可能不同取值的个数）很大的离散特征，例如IP地址、网站域名、城市名、家庭地址、街道、残品编号登，上述预处理方法效果往往不太好。因为对于自然数编码，简单模型容易欠拟合，而复杂模型容易过拟合；独热编码得到的特征矩阵太过稀疏。对于高基数类别变量，一种有效的方式是基于目标变量对类别特征进行编码，即有监督的编码。其适用于分类和回归问题。例如对于分类问题，采用交叉验证的方式，即将样本划分为5份，针对其中一份数据，计算离散特征的每个取值在另外4份数据中每个类别的比例。为了避免过拟合，也可以采用嵌套的交叉验证划分方法。 回归问题同样采用交叉验证的方式计算目标变量均值对类别变量编码。在实际问题中，我们往往利用历史数据来预测未来结果。一次我们一般基于时间信息来划分训练集合验证集，利用相同时间窗口大小的历史数据来对类别特征进行编码。例如，在广告点击率预测问题中，我们计算广告主ID在过去固定一段时间内的点击率，对广告主ID进行目标编码。目标编码方法对于基数较低的离散变量通常很有效，但对于基数特别高的离散变量可能会有过拟合的风险。因为很多类别特征的取值样本个数太少，不具有统计意义。对于这种情况，我们通常采用贝叶斯方法，即对统计特征进行贝叶斯平滑，如拉普拉斯平滑或者先验概率和后验概率加权平均的方式。

类别特征之间交叉组合。两个类别特征进行笛卡尔积操作可以产生新的类别特征，这种操作适用于两个类别特征的基数较小的情况，如果基数较大，交叉后基数太大，效果可能不好。也可以进行多个类别特征的交叉组合，根据实际需要可以进行二阶及以上的交叉组合，最后通过特征选择方法(在后面)选取重要的组合方式。特征组合还可以基于统计进行组合。例如针对城市ID和商品ID，可以计算某个城市有多少不同的商品ID以及当前ID出现次数的分布，或者计算某个城市出现次数最多的商品ID，从而得到新的类别特征。统计的方法同样可以用于多特征，例如针对年龄、性别、产品ID三个类别特征，可以计算某个年龄段不同性别的人购买过多少产品或者对当前产品ID购买次数的分布等。在实际应用中，类别特征之间的组合方式千变万化，这类特征一般从业务逻辑角度出发进行构造。相比于笛卡尔积基于分组统计的特征组合方式计算更加复杂，一般强依赖专业领域知识。

类别特征和数值特征之间也可以进行组合。通常是在类别特征某个类别中计算数值特征的一些统计量。例如针对用户ID，统计获取一段时间内在网站上的浏览次数、购买次数、以及购买价格的统计量，如均值、中位数、标准差、最大值和最小值登；针对产品，统计用户对产品的评分、评价次数、购买次数、浏览次数登。再比如，统计产品在某个区域的销量 、产品的价格，或者当前产品的价格跟产品所在区域内的平均价格的差价等。 这类特征也强依赖专业领域知识。这种组合方式也可以看做是利用数值特征对类别特征进行编码，与基于目标变量对类别变量编码的方法，不同之处是这里不需要划分训练集进行计算。

数值特征和类别特征是机器学习应用中最常见的两类特征，上面提到了关于这两类特征的一些常用的特征预处理技巧。基于这些技巧可以构造大量特征。但我们无法构造左右可能的特征表达式。既要考虑模型的使用成本，也要考虑到特征的构造成本。当然，我们可以通过特征选择选取重要特征，但特征选择成本也很高。因此，在实际应用中我们选择性的构造特征。对于不同类别特征采取哪一种或几种方法，则依赖于我们对业务和数据本身的理解以及对模型的理解。通过对数据内部机构和规律的探索性分析，可以找到跟目标变量相关的信息，进而根据模型需要的输入形式利用预处理技术对这些信息进行编码，即构造特征。

时间变量通常以时间点(年月日时分秒)或时间戳等形式表示，可以直接作为类别变量处理，类别特征的处理方式对时间特征同样适用。但时间变量还包含其他更丰富的信息，常用的有年、月、日、时、分、秒、星期几、一年过了多少天，一天过了多少分钟、季度、是否闰年、是否季度(月、周)初(末)、是否营业时间和是否节假日等。除了对单个时间变量的预处理外，根据具体业务对两个时间变量进行组合也能提取重要的特征。例如计算产品上面到现在过了多久，顾客上次借款到现在的时间间隔，两个时间间隔之间是否包含节假日或其他特殊日期等。
除了时间本身外，时间序列相关的特征是时间变量更重要的特征。时间序列除了一维时间变量，还包含一维其他变量，如股票价格、天气温度、降雨量、订单量等。时间序列分析的主要目的是基于历史数据来预测未来信息。对于时间序列，主要关心的是长期的变动趋势、周期性的变动(如季节性变动)以及不规则的变动。对于时间序列信息，当前时间点之前的信息通常很重要，例如滞后特征(lag特征)使用非常广泛。滞后特征是时间序列预测问题转化为监督学习问题的一种经典方法。若我们的问题是利用历史数据预测未来，则对于t时刻，可以将t-1、t-2和t-3时刻的值作为特征使用。若我们的为题可以考虑未来信息，则t+1、t+2和t+3时刻的值也可以作为特征使用。另一种有效方式是滑动窗口统计特征，例如计算前n个值均值(回归问题)，或者前n个值中每个类别的分布(分类问题)。时间窗口的选取可以有多种方式，滞后特征是滑动窗口统计的一种特例，对应时间窗口宽度是1。另一种常用的窗口设置包含所有历史数据，称为扩展窗口统计。

 基于空间位置的变量，如经纬度。对于经纬度，除了作为数值变量使用外，还可以用其他更加有效的使用方式。例如可以对经纬度做散列，从而对空间区域进行分块，得到一个类别特征，也可以通过坐标拾取系统获得当前位置的行政ID 、街道ID、城市ID等类别特征，进而利用类别特征的处理方式进行特征预处理。还可以计算两个位置之间的距离，如用户到快递自提点、用户到超市或餐厅的距离。距离的计算方式有很多种，可以用欧式距离、球面距离、曼哈顿距离，也可以是真实的街道距离。

时间变量和空间变量只是两种比较典型的变量，通过对这两种变量进行预处理可以进而转换为多个数值变量或类别变量。实际应用中还有很多类型的变量，从对数据本身理解等角度可以从其本身衍生构造出很多特征。

类别特征的处理同样适用于文本特征。自然语言要处理的对象是文本信息。对于文本特征，基于深度学习的自动特征工程效果越来越好，但好的特征仍然具有竞争力。 文本特征往往产生特别稀疏的特征矩阵。可以从以下几个方面进行预处理：字符转小写、分词、去除无用词，提取词根、拼写纠错、词干提取、标点符号编码、文档特征、实体插入和提取、Word2Vec、文本相似性、去除停止词、去除稀有词、TF-IDF、LDA、LSA等。

语料构建。构建一个由文档或短语组成的矩阵。矩阵的每一行为文档，可以理解为对产品的描述，每一列为单词。通常文档个数与样本个数一致。可对照搜索引擎中的倒链表数据结构理解。

文本清洗。如果数据通过网页抓取，首先要剔除HTML标记；停止词值用于语句构建不包含任何真实信息，需要剔除；通常为了避免大小写差异都转成小写；同意编码；去除标点符号；去除数字；去除空格；还原为词根。根据具体情况，文本可能不一定要清洗。例如某编辑对某物品的秒速，如果关心的对象是物品，则需要去除噪声保留关键信息，但如果关心的对象是编辑员，则噪声信息一定程度上反映了此编辑员的水平。

词语通常有三类重要的词性：名词、动词和形容词。形容词描述名词的属性。词性标注的目标是为文本中的每个词标注一个合适的词性，词性标注可以帮组我们了解语言的内在结构。

词形还原是把任何形式的语言词汇还原为一般形式(能表达完整语义)。词干提取是抽取词的词干和词根形式(不一定能表达完整语义)。两者都能有效归并词形。

文本统计特征是最简单的文本特征，他不需要考虑词序信息，他包括计算文本的长度、单词个数、数字个数、字母个数、大小写单词个数、大小写字母个数、标点符号个数、特殊字符个数等；数字占比、字母占比、特殊字符占比等；以及名词个数、动词个数等。

在自然语言处理中，N-Gram模型将文本转换为连续序列，序列的每一项包含n个元素(可以是单词或字母等元素)，例如"the dog smelled like a skunk"，得到3-Gram(the dog smelled,dog smelled like, smelled like a,like a skunk)。这种想法是为了将一个或者多个单词同时出现的多个信息喂给模型。为了更好的保留词序信息，构建更有效的语言模型，我们希望在N-Gram模型中选用更大的n。但是n很大时，数据会很稀疏。3-Gram是常用的选择。统计语言模型一般都是基于N-Gram的统计估计条件概率，基于神经网络的语言模型也是对N-Gram进行建模。

词集模型。机器学习模型不能直接处理文本，因此我们需要将文本(或N-Gram序列)转化为实数或者实向量。在词集模型中，向量的每个分量的取值为0和1，代表单词是否在文档中出现。向量空间模型没有考虑词序信息。

在词集模型中，向量的取值不考虑单词出现的次数，这会损失很多信息。词袋模型中，向量的每个分量的取值为单词在文档中的词频，为了避免向量的维度太大，通常会过滤掉在文档集合中词频很小的单词。

信息检索是当前应用十分广泛的一种技术，论文检索、搜索引擎都属于信息检索的范畴。通常，人们把信息检索问题抽象为：在文档集合D上，对于由关键词w[1] ... w[k]组成的查询串q，返回一个按查询q和文档d匹配度relevance(q, d)排序的相关文档列表D'。先后出现了布尔模型、向量模型等
https://www.cnblogs.com/weidagang2046/archive/2012/10/22/tf-idf-from-probabilistic-view.html
TF（Term Frequency，词频）、IDF（Inverse Document Frequency，逆文档频率），用来评估单词对于文件集或语料库中的其中一份文件的重要程度。单词或短语的重要性与它在文件中出现的频率成正比，与它在语料库中出现的频率成反比(如lucene默认排序规则)。假设词汇表有N个词，文档d对应的向量表示V<sub>d</sub>=[W<sub>1,d</sub>, W<sub>2,d</sub>,  ... ,W<sub>N,d</sub]<sup>T</sup>
TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或短语具有很好的类别区分能力，适合用来分类。TF-IDF模型是经典的向量空间模型（Vector Space Model，VSM），我们可以基于文档的向量表示计算文档之间的相似度，但不能很好的表示特别长的文档，而且这种向量表示也没有考虑词序信息。基于TF-IDF和词袋模型得到的表示文本的向量往往维度非常大，因此实际应用中一般需要降维处理。（维度例子，向量例子，前面向量模型）

在信息检索中，我们往往需要计算检索词和文档之间的相关性。例如将检索词和文档都表示为向量，计算两个向量之间的余弦相似度。查百度百科

另外一种常用的相似度是Jaccard，它是两个文档中交集(∩)的单词个数除以两个文档出现单词的总和

编辑距离指两个字符串由一个转成另外一个所需要的最少编辑操作(如插入、删除、替换)次数，它也是衡量两个字符串相似度的指标。在自然语言处理中，单词一般作为基本的处理单元。

隐性语义分析是把高维的向量空间模型表示的文档映射到低维的潜在语义空间中(例子)。隐性语义分析采用将文档或词矩阵进行奇异值分解(Singular Value Docomposition,SVD)的方法。由于奇异值分解的方法本身是对文档特征进行排序，我们可以通过限制奇异值个数对数据进行降噪和降维。一般而言，文档与文档之间或文档和查询之间的相似性在简化的潜在语义空间的表达更为可靠。

Word2Vec是最常见的一种单词嵌入，将单词坐在的空间(高维)映射到一个低维的向量空间中，这样每个单词对应一个向量，通过计算向量之间的余弦相似度就可以得到某个单词的同义词。传统的单词表示，如独热编码，仅仅是将词转化为数字表示，不包含任何语义信息。而单词嵌入包含了单词的语义信息，这类表示称为分布式表示(?)。

======（与上面关联，新建一篇）特征提取是从原始数据中构造新特性，与之不同，特征选择是从这些特征集合中选出一个子集。特征选择也称为属性选择或变量选择，是指为了构建模型二选择相关特征子集的过程。特征选择的目的有三个：   
简化模型，使之更易理解。   
改善性能。   
改善通用性、降低过拟合风险。

[edit](https://github.com/saaavsaaa/saaavsaaa.github.io/edit/master/aaa/Feature_Extraction.md)

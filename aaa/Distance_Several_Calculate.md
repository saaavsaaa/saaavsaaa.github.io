机器学习中的数值计算主要集中在两方面：概率和距离。距离可以用来衡量信息之间的差异程度，可以用作计算结果的衡量标准。常用的主要有：
欧式距离：这个大家都熟悉，欧式几何都是直角坐标系，勾股定理求距离，小学还是初中的几何，公式就不写了。缺点是特征之间的距离都是独立计算，两组计算之间没有联系，依赖测量单位，且不能体现出不同特征的权重。


https://en.wikipedia.org/wiki/Mahalanobis_distance
马氏距离：由印度统计学家马哈拉诺比斯(P. C. Mahalanobis)提出的，表示数据的协方差距离。

这里先说一下协方差：协方差表示两个变量(两个维度，协方差矩阵计算的是不同维度之间的协方差，而不是不同样本之间的)期望值的变化差异。当两个变量相同时，协方差就变成方差了，方差反应数据变化幅度或者说稳定程度，单个变量的原始数据与平均值的差异。协方差表示两个变量的变化趋向之间的差异，注意，不是数值差异，是趋向的差异。一个单调减一个单调增那么这两个变量间的协方差就是负值；如果两个变量值都大于或都小于自身的期望值，那么它们之间的协方差就是正值。

马氏距离是一种有效的计算两个未知样本集的相似度的方法。它考虑到特性之间的联系，如身高的信息与体重的信息是有关联的。同时它与测量尺度层次无关 。

对于一组矢量{\displaystyle {\vec {x}}=(x_{1},x_{2},x_{3},\dots ,x_{N})^{T}} ，设它的平均值为{\displaystyle {\vec {\mu }}=(\mu _{1},\mu _{2},\mu _{3},\dots ,\mu _{N})^{T}}且协方差为S，则它的马氏距离为{\displaystyle D_{M}({\vec {x}})={\sqrt {({\vec {x}}-{\vec {\mu }})^{T}S^{-1}({\vec {x}}-{\vec {\mu }})}}.\,}

协方差矩阵s同一分布的两个随机向量x和y的相异性测度 
马氏距离也可以定义为两个服从同一分布且其协方差矩阵为S的随机向量{\vec {x}}和{\vec {y}}之间的差异程度 {d(\vec{x},\vec{y})=\sqrt{(\vec{x}-\vec{y})^T S^{-1} (\vec{x}-\vec{y})}.\,}。

如果协方差矩阵为单位矩阵，那么马氏距离就简化为欧氏距离。如果协方差矩阵为对角阵，则可称为正规化的欧氏距离 d(\vec{x},\vec{y})=\sqrt{\sum_{i=1}^N  {(x_i - y_i)^2 \over s_{i}^2}}，其中Si是样本集中x_i和y_i的标准差。

马氏距离的计算是建立在总体样本的基础上的，也就是说，如果拿同样的两个样本，放入两个不同的总体中，最后计算得出的两个样本间的马氏距离通常是不相同的，除非这两个总体的协方差矩阵碰巧相同；

在计算马氏距离过程中，要求总体样本数大于样本的维数，否则得到的总体样本协方差矩阵逆矩阵不存在(就像求多元一次方程，一般方程个数少于未知数是求不出值的，同样，方程个数够，但是一次方程的直线如果无交点也求不出，所以样本点共线情况也可能导致协方差矩阵不存在)
马氏距离的计算是不稳定的，不稳定的来源是协方差矩阵，这也是马氏距离与欧氏距离的最大差异之处
优点：它不受量纲的影响，两点之间的马氏距离与原始数据的测量单位无关，由标准化数据和中心化数据(即原始数据与均值之差）计算出的二点之间的马氏距离相同。马氏距离还可以排除变量之间的相关性的干扰。
缺点：它的缺点是夸大了变化微小的变量的作用。



曼哈顿距离
出租车几何或曼哈顿距离（Manhattan Distance）是由十九世纪的赫尔曼·闵可夫斯基所创词汇，是种使用在几何度量空间的几何学用语，用以标明两个点在标准坐标系上的绝对轴距总和。d（i，j）=|xi-xj|+|yi-yj|，关于这个我第一反应是排列组合的格路模型，只不过那个是求有多少条路，这个是求路径长度。曼哈顿距离依赖坐标系的旋转。在出租车几何学中，一个圆是由从圆心向各个固定曼哈顿距离标示出来的点围成的区域。因此这种圆其实就是旋转了45度的正方形。如果有一群圆，任两圆皆相交，则整群圆必在某点相交；因此曼哈顿距离会形成一个超凸度量空间（Injective metric space）。对一个半径为r 的圆来说，这个正方形的圆每边长√2r。此'"圆"的半径r对切比雪夫距离 (L∞ 空间)的二维平面来说，也是一个对座标轴来说边长为2r的正方形，因此二维切比雪夫距离可视为等同于旋转且放大过的二维曼哈顿距离。然而这种介于L1与L∞的相等关系并不能延伸到更高的维度。



契比雪夫距离
闵式距离
海明距离


为了Kaldi，由于SGE收费，所以选择搭建slurm集群，花了好些时间，记录一下还记得的问题。

服务器的时间要同步。
如果DHCP有问题，就要配好hosts，总之机器名和IP都要通

munge，我直接用的ubuntu的apt-get安装的，装上就行
配置方面，只要保证集群中所有机器的munge.key是相同的就可以了
启动时，要保证启动 munge 的用户一致，用户的uid之类的，不然启动 slurm 时会报错：Unable to register with slurm controller
官网建议的权限：
  sudo chmod 0700 /var/log/munge
  sudo chmod 0700 /var/log/munge/*
  sudo chmod 0755 /run/munge

如果是自己编译的，注意配置的路径前缀，它会编译进libmunge里，如果写配置文件的时候和它不一样，比如pid文件，改任何配置文件都是不会好的。只有把这个程序集删掉重新来才行，注意 make uninstall 可能不会删，可能需要手动删除。

不知道是不是 slurm 太小众了，ubuntu 的官方源里没有，换了清华的源差了好几个大版本，不过反正都能用
如果用apt装的，需要自己找包里带的配置文件例子改一下，官方网站上只能生成最新版的配置，好多属性对不上
权限问题，启动会提醒的，要把一些目录 chown -R 成启动用户所有，另外，sudo chmod 0755 /var/lib/slurm/slurmd
新加入集群的节点看状态如果dwon着，可以scontrol update NodeName=xxxxx State=RESUME

使用cgroup的时候要注意系统带的一些它相关的依赖的版本是不是兼容，可以apt-get install xxx=指定版本

https://slurm.schedmd.com/quickstart_admin.html#Config
https://slurm.schedmd.com/configurator.htm

slurmd -Dvvvvvv v的个数代表debug级别

$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            1210_1   compute make_mfc  manager  R       0:33      1 SL311A-T-Speech2
$ sinfo 
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
compute*     up   infinite      1  alloc SL311A-T-Speech2
compute*     up   infinite      1   idle sl311a-t-speech3
control      up   infinite      1   idle SL311A-T-Speech1

只有一个节点正常执行了     
另一个节点日志：     
slurmd: debug3: in the service_connection
slurmd: debug2: got this type of message 4005
slurmd: debug2: Processing RPC: REQUEST_BATCH_JOB_LAUNCH
slurmd: task_p_slurmd_batch_request: 1211
slurmd: debug3: task/affinity: job 1211 core mask from slurmctld: 0x3
slurmd: task/affinity: job 1211 CPU input mask for node: 0x0003
slurmd: debug3: _lllp_map_abstract_masks
slurmd: task/affinity: job 1211 CPU final HW mask for node: 0x0101
slurmd: debug3: state for jobid 11: ctime:1589363100 revoked:0 expires:0
slurmd: debug3: state for jobid 12: ctime:1589363108 revoked:0 expires:0
slurmd: debug3: state for jobid 13: ctime:1589363111 revoked:0 expires:0
slurmd: debug3: state for jobid 14: ctime:1589363119 revoked:0 expires:0
slurmd: debug3: state for jobid 15: ctime:1589363422 revoked:0 expires:0
slurmd: debug3: state for jobid 16: ctime:1589363430 revoked:0 expires:0
slurmd: debug3: state for jobid 17: ctime:1589363436 revoked:0 expires:0
slurmd: debug3: state for jobid 18: ctime:1589423368 revoked:0 expires:0
slurmd: debug3: state for jobid 19: ctime:1589434407 revoked:0 expires:0
slurmd: debug3: state for jobid 20: ctime:1589434411 revoked:0 expires:0
slurmd: debug3: state for jobid 22: ctime:1589535335 revoked:0 expires:0
slurmd: debug3: state for jobid 23: ctime:1589766689 revoked:0 expires:0
slurmd: debug3: state for jobid 24: ctime:1589768149 revoked:0 expires:0
slurmd: debug3: state for jobid 25: ctime:1589770622 revoked:0 expires:0
slurmd: debug3: state for jobid 52: ctime:1589959961 revoked:0 expires:0
slurmd: debug3: state for jobid 1206: ctime:1595233974 revoked:0 expires:0
slurmd: debug3: state for jobid 1207: ctime:1595234075 revoked:0 expires:0
slurmd: debug3: state for jobid 1208: ctime:1595234088 revoked:0 expires:0
slurmd: debug3: state for jobid 1209: ctime:1595234251 revoked:1595234251 expires:1595234251
slurmd: debug3: state for jobid 1209: ctime:1595234251 revoked:0 expires:0
slurmd: _run_prolog: run job script took usec=4
slurmd: _run_prolog: prolog with lock for job 1211 ran for 0 seconds
slurmd: get env for user manager here
slurmd: Launching batch job 1211 for UID 1000
slurmd: debug3: _rpc_batch_job: call to _forkexec_slurmstepd
slurmd: debug3: slurmstepd rank -1 (SL311A-T-Speech2), parent rank -1 (NONE), children 0, depth 0, max_depth 0
slurmd: debug3: _send_slurmstepd_init: call to getpwuid_r
slurmd: debug3: _send_slurmstepd_init: return from getpwuid_r
slurmd: debug3: _rpc_batch_job: return from _forkexec_slurmstepd: 0
